{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81062.51s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Requirement already satisfied: numpy in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "81069.84s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Requirement already satisfied: pandas in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "81076.76s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Requirement already satisfied: matplotlib in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mba_2023_nyb/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point while writing this i accidentally deleted the file and had to rewrite parts of it. \n",
    "You can do ur best guess on when this happened by looking at the amount of comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let movies be a seperate array for easier access later.\n",
    "movies = [\"The Dark Knight\", \"Interstellar\", \"Star-wars\", \"Løvernes Konge\", \"De urørlige\", \"The prestige\"]\n",
    "\n",
    "# fact_table = [\n",
    "#     [\"\",          *movies], # Unwrap the movies array with spread operator.\n",
    "#     [\"Anton\",     4, 5, 4, 2, 3, 4],\n",
    "#     [\"Benjamin\",  3, 2, 2, 4, 5, 3],\n",
    "#     [\"Mads\",      4, 2, 4, 3, 3, 5],\n",
    "#     [\"Joachim\",   4, 4, 5, 3, 3, 4],\n",
    "#     [\"Rasmus\",    2, 1, 2, 5, 4 ,3],\n",
    "#     [\"Peter\",     5, 5, 3, 1, 2, 4],\n",
    "# ]\n",
    "\n",
    "fact_table = [\n",
    "    [\"\",          *movies], # Unwrap the movies array with spread operator.\n",
    "    [\"Anton\",     4, 5, 4, 2, 3, 4],\n",
    "    [\"Benjamin\",  3, 2, 2, 4, 5, 3],\n",
    "    [\"Mads\",      4, 2, 4, 3, 3, 5],\n",
    "    [\"Joachim\",   4, 4, 5, 3, 3, 4],\n",
    "    [\"Rasmus\",    2, 1, 2, 5, 4 ,3],\n",
    "    [\"Peter\",     5, 5, 3, 1, 2, 4],\n",
    "]\n",
    "\n",
    "\n",
    "# Makes a mapping from index space of fact_matrix to human readable labels and inverse.\n",
    "# (The * operator here is called the spread operator and the syntax for creating dicts and lists here is called list comprehension)\n",
    "users = [ user for user in (fact_table[i][0] for i in range(len(fact_table)))]\n",
    "users = { users[i+1]: i for i in range(len(users[1:])) } # We downshift index mappings by 1 because indexes start from 0 not 1.\n",
    "inv_users = { v: k for (k,v) in users.items() }\n",
    "\n",
    "# Set up the fact table to a pandas dataframe.\n",
    "fact_matrix = pd.DataFrame(fact_table)\n",
    "fact_matrix.header = fact_matrix.iloc[0:1] # sets the header to the first row \n",
    "fact_matrix.columns = fact_matrix.iloc[0] # Give the correct headers\n",
    "fact_matrix = fact_matrix[1:] # Remove the header from row space\n",
    "fact_matrix[movies] # Perfect matrix presentation.\n",
    "\n",
    "def prettier_dataframe_to_latex(df: pd.DataFrame, noHead = False, noIndex = False) -> str:\n",
    "    result = df.style\n",
    "\n",
    "    if noIndex and noHead:\n",
    "        # Remove header row and index column\n",
    "        result = df.style.format(\"{:.1f}\").hide().hide(axis=1)\n",
    "    else: \n",
    "        if noIndex:\n",
    "            # Remove index column\n",
    "            result = df.style.format(\"{:.1f}\").hide()\n",
    "        if noHead:\n",
    "            # Remove header\n",
    "            result = df.style.format().hide(axis=1)\n",
    "\n",
    "    result = result.format(escape=\"latex\").to_latex().replace(\"tabular\",\"pmatrix\")\n",
    "    return  re.sub(r'\\{l+\\}', \"\", result) # Regex away the leftover column space rendering hints from tabular output element\n",
    "\n",
    "# You can uncomment each line to see what they look like or simply browse the next section.\n",
    "# print(prettier_dataframe_to_latex(fact_matrix))\n",
    "# print(prettier_dataframe_to_latex(fact_matrix[movies]))\n",
    "# print(prettier_dataframe_to_latex(fact_matrix, noHead=True, noIndex=True))\n",
    "# print(prettier_dataframe_to_latex(fact_matrix[movies], noHead=True, noIndex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_similarity(vec_a: np.array, vec_b: np.array) -> float:\n",
    "    # Returns the pearson similarity between two vectors.\n",
    "    vec_a = vec_a - vec_a.mean() # Compute mean norm of vec_a\n",
    "    vec_b = vec_b - vec_b.mean() # Compute mean norm of vec_b\n",
    "    return np.dot(vec_a, vec_b) / (LA.norm(vec_a) * LA.norm(vec_b))\n",
    "\n",
    "def manthattan_similarity(vec_a: np.array, vec_b: np.array) -> float:\n",
    "    # Returns the manhattan similarity between two vectors\n",
    "  return  1/ (1+ np.sum(np.abs(vec_a - vec_b)))\n",
    "\n",
    "def eucledian_similarity(vec_a, vec_b):\n",
    "    # Returns the eucledian similarity between two vectors\n",
    "    return 1 / (1 + np.sqrt(np.sum(np.power(vec_a - vec_b,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_prediction(fact_matrix: np.matrix, user_movie_subset: np.matrix) -> np.matrix:\n",
    "    prediciton_matrix = np.empty(fact_matrix[movies].shape)\n",
    "    for i in range(fact_matrix[movies].shape[0]):\n",
    "        rows = []\n",
    "        for k in user_movie_subset[str(i)+\"-idx\"]:\n",
    "            rows.append(\n",
    "                fact_matrix[movies].iloc[k,:]\n",
    "            )\n",
    "        # Compute average of rows by column.\n",
    "        averages_of_movies_from_nearest_neighbours = pd.DataFrame(rows).mean(axis=0)\n",
    "        for j in range(len(averages_of_movies_from_nearest_neighbours)):\n",
    "            prediciton_matrix[i,j] = averages_of_movies_from_nearest_neighbours[j]\n",
    "\n",
    "    return prediciton_matrix\n",
    "\n",
    "def weighted_average_prediction(fact_matrix: np.matrix, user_movie_subset: np.matrix) -> np.matrix:\n",
    "    # Here i want to elementwise multiply the weight with the output of the average_prediction function.\n",
    "    prediciton_matrix = np.empty(fact_matrix[movies].shape)\n",
    "    for i in range(fact_matrix[movies].shape[0]):\n",
    "        similarity_sum = user_movie_subset[str(i)].sum()\n",
    "        weight_index_dict = {}\n",
    "        for k in range(len(user_movie_subset[str(i)])):\n",
    "            weight_index_dict[user_movie_subset[str(i)+\"-idx\"][k]] = user_movie_subset[str(i)][k] / similarity_sum\n",
    "        # Compute weighted rating of movies from nearest neighbours.\n",
    "        for key in weight_index_dict.keys():\n",
    "            prediciton_matrix[i,:] += fact_matrix[movies].iloc[key,:] * weight_index_dict[key]\n",
    "                        \n",
    "    return prediciton_matrix\n",
    "\n",
    "def weighted_average_corrected_prediction(fact_matrix: np.matrix, user_movie_subset: np.matrix) -> np.matrix:\n",
    "    prediciton_matrix = np.empty(fact_matrix[movies].shape)\n",
    "    for i in range(fact_matrix[movies].shape[0]):\n",
    "        similarity_sum = user_movie_subset[str(i)].sum()\n",
    "        weight_index_dict = {}\n",
    "        for k in range(len(user_movie_subset[str(i)])):\n",
    "            weight_index_dict[user_movie_subset[str(i)+\"-idx\"][k]] = user_movie_subset[str(i)][k] / similarity_sum\n",
    "        # Compute weighted rating of movies from nearest neighbours.\n",
    "        for key in weight_index_dict.keys():\n",
    "            Ra_avg = np.mean(fact_matrix[movies].iloc[i,:]) \n",
    "            Rb_avg = np.mean(fact_matrix[movies].iloc[key,:]) \n",
    "            # print(Ra_avg)\n",
    "            # print((fact_matrix[movies].iloc[key,:] - Rb_avg) * weight_index_dict[key])\n",
    "            prediciton_matrix[i,:] += (Ra_avg / len(weight_index_dict)) + ((fact_matrix[movies].iloc[key,:] - Rb_avg) * weight_index_dict[key])\n",
    "                        \n",
    "    return prediciton_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(input_matrix: np.matrix, similarity_function: callable, column_major = True) -> np.matrix:\n",
    "    # Computes the similarity matrix for the input matrix.\n",
    "    similarity_matrix = np.identity((input_matrix.shape[0])) # Make the matrix N x N with a diagonal of 1 as users are perfectly similar to themself\n",
    "\n",
    "    for i in range(input_matrix.shape[0]):\n",
    "        for j in range(input_matrix.shape[0]):\n",
    "            if i==j:\n",
    "                # Skips current iteration because the result is known beforehand to be 1.\n",
    "                # Could also speed up the computation by a factor of 2 by recognising that these matrices are all symmetric around the diagonal because of the \n",
    "                # commutative property of the similarity measures used. In which case we could use the break keyword instead followed by a transpose statement in the output.\n",
    "                continue\n",
    "            # if column major it will apply two columns and find the similarity between those two\n",
    "            if column_major: \n",
    "                similarity_matrix[i, j] = similarity_function(input_matrix[movies[i]], input_matrix[movies[j]])\n",
    "            # If row major it will apply the same but to different rows\n",
    "            else:\n",
    "                similarity_matrix[i, j] = similarity_function(input_matrix[movies].iloc[i], input_matrix[movies].iloc[j])\n",
    "    resulting_matrix = np.matrix(similarity_matrix)\n",
    "    # This can be skipped if you use continue instead of break earlier. But what would the fun in that be?\n",
    "    return resulting_matrix\n",
    "    # return resulting_matrix + resulting_matrix.transpose() - np.identity(resulting_matrix.shape[0]) # Slightly faster implementation for the lols\n",
    "\n",
    "# You will notice that this step is left out of the homework assignment but \n",
    "# programatically this works out sort of like glue which allows you to go \n",
    "# from one step to another without overly complicating the other functions \n",
    "# which i would really prefer to just do one thing onto itself, one function at a time.\n",
    "def knn_tuple(similarity_matrix: np.matrix, k=1) -> np.array:\n",
    "    # Maps every index with their k highest value indices\n",
    "    temp_sorted_arrays = {}\n",
    "    sm_df = pd.DataFrame(similarity_matrix)\n",
    "\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        # The syntax here is quite werid because i use a chain of python tricks\n",
    "        # And type castings to access functions that id rather use over others\n",
    "        # The result is the list of k-most nearest neighbours\n",
    "        temp_sorted_array = sm_df.iloc[i].sort_values()[::-1].array[1:]\n",
    "        temp_sorted_arrays[str(i)] = temp_sorted_array[0:k]\n",
    "        element_wise_mapping = []\n",
    "        for j in temp_sorted_arrays[str(i)]:\n",
    "            # Technically here we are assuming that this function is invertible.\n",
    "            # Failing this assumption we will ignore two neighbours of same distance\n",
    "            # Which is not always true, it just so happens that it becomes true more often with \n",
    "            # more features (movies) or if the distance happens to be equal to two neighbours\n",
    "            # whose data is also equal.\n",
    "            # No such instances of equidistant neighbours occur with out dataset.\n",
    "            element_wise_mapping.append(list(sm_df.iloc[i]).index(j))\n",
    "        temp_sorted_arrays[str(i)+\"-idx\"] = element_wise_mapping\n",
    "\n",
    "    # This returns two things. A list of the k highest values and their index on the row space of the similarity matrix\n",
    "    # You can feed this index into inv_users for names or use iloc calls to fact_matrix to generate predictions.\n",
    "    # Maps kind of like this: distances -> user <- users\n",
    "    return temp_sorted_arrays\n",
    "\n",
    "def prediction_matrix(fact_matrix: np.matrix, knn_tuple: dict, prediction_funciton: callable) -> np.matrix:\n",
    "    # Computes the prediction matrix for the input matrix given by similarity matrix + some KNN regieme.#\n",
    "    return prediction_funciton(fact_matrix, knn_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(fact_matrix: np.matrix, prediction_matrix: np.matrix) -> float:\n",
    "    error_matrix = fact_matrix[movies] - prediction_matrix\n",
    "    return np.sum(np.sum(np.abs(error_matrix)) / np.count_nonzero(error_matrix))\n",
    "\n",
    "def mean_absolute_error_squared(fact_matrix: np.matrix, prediction_matrix: np.matrix) -> float:\n",
    "    error_matrix = fact_matrix[movies] - prediction_matrix\n",
    "    return np.sum(np.sum(np.power(error_matrix,2)) / np.count_nonzero(error_matrix))\n",
    "\n",
    "def absolute_mean_residual_squared(fact_matrix: np.matrix, prediction_matrix: np.matrix) -> float:\n",
    "    error_matrix = (fact_matrix[movies] - prediction_matrix)**2\n",
    "    return np.sum(np.sum(error_matrix) / np.count_nonzero(error_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = {}\n",
    "\n",
    "error_functions = [mean_absolute_error, mean_absolute_error_squared, absolute_mean_residual_squared]\n",
    "prediction_functions = [average_prediction, weighted_average_prediction, weighted_average_corrected_prediction]\n",
    "similarity_functions = [pearson_similarity, manthattan_similarity, eucledian_similarity]\n",
    "fn_name_from_index = lambda fn_list, k:  str(fn_list[k].__str__().split(\" \")[1])\n",
    "# print(fn_name_from_index(error_functions, 1))\n",
    "\n",
    "def compute_predictions(error_function: callable, prediction_function: callable, similarity_function: callable, fact_matrix: np.matrix, k_nearest: int) -> float:\n",
    "    # This function is very much a case of just call one method after another. Here you see why i write descriptive names for my variables and arguments.\n",
    "    sim_mat = similarity_matrix(\n",
    "        fact_matrix, \n",
    "        similarity_function=similarity_function,\n",
    "        column_major=False\n",
    "    )\n",
    "    knn = knn_tuple(\n",
    "        similarity_matrix=sim_mat,\n",
    "        k=k_nearest\n",
    "    )\n",
    "    pred_mat = prediction_matrix(\n",
    "        fact_matrix=fact_matrix,\n",
    "        knn_tuple=knn,\n",
    "        prediction_funciton=prediction_function\n",
    "    )\n",
    "    return error_function(fact_matrix=fact_matrix, prediction_matrix=pred_mat)\n",
    "\n",
    "def grid_search(fact_matrix):\n",
    "    global gs_results # call global here to exit block scope of grid_search()\n",
    "    # 3^3*5 = 135 experiments\n",
    "    for i in range(len(error_functions)):\n",
    "        for j in range(len(prediction_functions)):\n",
    "            for s in range(len(similarity_functions)):\n",
    "                for k in range(1,5):\n",
    "                    gs_results[\n",
    "                        f\"err={fn_name_from_index(error_functions, i)} pre={fn_name_from_index(prediction_functions,j)} sim={fn_name_from_index(similarity_functions,s)} k-near={k}\"\n",
    "                    ] = compute_predictions(\n",
    "                        error_function=error_functions[i],\n",
    "                        prediction_function=prediction_functions[j],\n",
    "                        similarity_function=similarity_functions[s],\n",
    "                        fact_matrix=fact_matrix,\n",
    "                        k_nearest=k\n",
    "                    )\n",
    "    return gs_results\n",
    "\n",
    "grid_search(fact_matrix)\n",
    "\n",
    "# Save gs_results to file\n",
    "    \n",
    "\n",
    "pd.DataFrame(gs_results, index=[0]).sort_values(by=0, axis=1).to_csv(\"gs_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333331"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sim = similarity_matrix(fact_matrix, similarity_function=pearson_similarity, column_major=False)\n",
    "predmat = prediction_matrix(fact_matrix, knn_tuple(test_sim, k=3), prediction_funciton=average_prediction)\n",
    "mean_absolute_error(fact_matrix,predmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157295698813058"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sim = similarity_matrix(fact_matrix, similarity_function=pearson_similarity, column_major=False)\n",
    "knn = knn_tuple(test_sim, k=3)\n",
    "\n",
    "weighted_average_prediction(\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{pmatrix}{lr}\n",
      " & 0 \\\\\n",
      "avg-euc & 3.000000 \\\\\n",
      "wavg-euc & 3.480107 \\\\\n",
      "wcavg-euc & 4.188129 \\\\\n",
      "avg-pearson & 3.000000 \\\\\n",
      "wavg-pearson & -10.247924 \\\\\n",
      "wcavg-pearson & -12.084611 \\\\\n",
      "\\end{pmatrix}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/6fd8hwdx18z6pbm4tp805qzh0000gn/T/ipykernel_81166/4191723395.py:13: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  XAnton.header = XAnton.iloc[0:1] # sets the header to the first row\n",
      "/var/folders/fp/6fd8hwdx18z6pbm4tp805qzh0000gn/T/ipykernel_81166/4191723395.py:28: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  XBenjamin.header = XBenjamin.iloc[0:1] # sets the header to the first row\n",
      "/var/folders/fp/6fd8hwdx18z6pbm4tp805qzh0000gn/T/ipykernel_81166/4191723395.py:43: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  XMads.header = XMads.iloc[0:1] # sets the header to the first row\n",
      "/var/folders/fp/6fd8hwdx18z6pbm4tp805qzh0000gn/T/ipykernel_81166/4191723395.py:58: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  XJoachim.header = XJoachim.iloc[0:1] # sets the header to the first row\n"
     ]
    }
   ],
   "source": [
    "# Just manually using the functions to compute the results for the report.\n",
    "\n",
    "XAnton = [\n",
    "    [\"\",          *movies], # Unwrap the movies array with spread operator.\n",
    "    [\"Anton\",     4, 0, 4, 2, 3, 4],\n",
    "    [\"Benjamin\",  3, 2, 2, 4, 5, 3],\n",
    "    [\"Mads\",      4, 2, 4, 3, 3, 5],\n",
    "    [\"Joachim\",   4, 4, 5, 3, 3, 4],\n",
    "    [\"Rasmus\",    2, 1, 2, 5, 4 ,3],\n",
    "    [\"Peter\",     5, 5, 3, 1, 2, 4],\n",
    "]\n",
    "XAnton = pd.DataFrame(fact_table)\n",
    "XAnton.header = XAnton.iloc[0:1] # sets the header to the first row \n",
    "XAnton.columns = XAnton.iloc[0] # Give the correct headers\n",
    "XAnton = XAnton[1:] # Remove the header from row space\n",
    "XAnton[movies] # Perfect matrix presentation.\n",
    "\n",
    "XBenjamin = [\n",
    "    [\"\",          *movies], # Unwrap the movies array with spread operator.\n",
    "    [\"Anton\",     4, 5, 4, 2, 3, 4],\n",
    "    [\"Benjamin\",  3, 2, 0, 4, 5, 3],\n",
    "    [\"Mads\",      4, 2, 4, 3, 3, 5],\n",
    "    [\"Joachim\",   4, 4, 5, 3, 3, 4],\n",
    "    [\"Rasmus\",    2, 1, 2, 5, 4 ,3],\n",
    "    [\"Peter\",     5, 5, 3, 1, 2, 4],\n",
    "]\n",
    "XBenjamin = pd.DataFrame(fact_table)\n",
    "XBenjamin.header = XBenjamin.iloc[0:1] # sets the header to the first row \n",
    "XBenjamin.columns = XBenjamin.iloc[0] # Give the correct headers\n",
    "XBenjamin = XBenjamin[1:] # Remove the header from row space\n",
    "XBenjamin[movies] # Perfect matrix presentation.\n",
    "\n",
    "XMads = [\n",
    "    [\"\",          *movies], # Unwrap the movies array with spread operator.\n",
    "    [\"Anton\",     4, 5, 4, 2, 3, 4],\n",
    "    [\"Benjamin\",  3, 2, 2, 4, 5, 3],\n",
    "    [\"Mads\",      4, 2, 4, 3, 3, 0],\n",
    "    [\"Joachim\",   4, 4, 5, 3, 3, 4],\n",
    "    [\"Rasmus\",    2, 1, 2, 5, 4 ,3],\n",
    "    [\"Peter\",     5, 5, 3, 1, 2, 4],\n",
    "]\n",
    "XMads = pd.DataFrame(fact_table)\n",
    "XMads.header = XMads.iloc[0:1] # sets the header to the first row \n",
    "XMads.columns = XMads.iloc[0] # Give the correct headers\n",
    "XMads = XMads[1:] # Remove the header from row space\n",
    "XMads[movies] # Perfect matrix presentation.\n",
    "\n",
    "XJoachim = [\n",
    "    [\"\",          *movies], # Unwrap the movies array with spread operator.\n",
    "    [\"Anton\",     4, 5, 4, 2, 3, 4],\n",
    "    [\"Benjamin\",  3, 2, 2, 4, 5, 3],\n",
    "    [\"Mads\",      4, 2, 4, 3, 3, 5],\n",
    "    [\"Joachim\",   0, 4, 5, 3, 3, 4],\n",
    "    [\"Rasmus\",    2, 1, 2, 5, 4 ,3],\n",
    "    [\"Peter\",     5, 5, 3, 1, 2, 4],\n",
    "]\n",
    "XJoachim = pd.DataFrame(fact_table)\n",
    "XJoachim.header = XJoachim.iloc[0:1] # sets the header to the first row \n",
    "XJoachim.columns = XJoachim.iloc[0] # Give the correct headers\n",
    "XJoachim = XJoachim[1:] # Remove the header from row space\n",
    "XJoachim[movies] # Perfect matrix presentation.\n",
    "\n",
    "# print(\n",
    "#   \"$$\"+\n",
    "#   \"UU_{pearson }(M_{joa})= \"+\n",
    "#   prettier_dataframe_to_latex(pd.DataFrame(similarity_matrix(XJoachim,pearson_similarity)))\n",
    "#   +\"$$\"\n",
    "#   )\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#   {\n",
    "#     \"people\":knn_tuple(similarity_matrix(XJoachim,pearson_similarity), k=3)[\"3-idx\"],\n",
    "#     \"similarity\": knn_tuple(similarity_matrix(XJoachim,pearson_similarity), k=3)[\"3\"],\n",
    "#     \"movie X\": XJoachim[movies].iloc[knn_tuple(similarity_matrix(XJoachim,pearson_similarity), k=3)[\"3-idx\"],0]\n",
    "#   }\n",
    "# )\n",
    "# name = \"joa\"\n",
    "# algo = \"pearson\"\n",
    "# print(f\"KNN_{ {'3,' +name} }(UU_{ {algo} }(M_{ {name} }))\")\n",
    "# print(prettier_dataframe_to_latex(df))\n",
    "X_pers = 3\n",
    "X_Movie = 3\n",
    "FMat = XJoachim\n",
    "idx = knn_tuple(similarity_matrix(FMat,eucledian_similarity), k=3)[f\"{X_pers}-idx\"]\n",
    "pred_mat_avg = prediction_matrix(\n",
    "  FMat,\n",
    "  knn_tuple(similarity_matrix(FMat,eucledian_similarity), k=3),\n",
    " average_prediction\n",
    ")\n",
    "pred_wmat_avg = prediction_matrix(\n",
    "  FMat,\n",
    "  knn_tuple(similarity_matrix(FMat,eucledian_similarity), k=3),\n",
    " weighted_average_prediction\n",
    ")\n",
    "pred_wcmat_avg = prediction_matrix(\n",
    "    FMat,\n",
    "  knn_tuple(similarity_matrix(FMat,eucledian_similarity), k=3),\n",
    " weighted_average_corrected_prediction\n",
    ")\n",
    "\n",
    "pears_pred_mat_avg = prediction_matrix(\n",
    "  FMat,\n",
    "  knn_tuple(similarity_matrix(FMat,pearson_similarity), k=3),\n",
    " average_prediction\n",
    ")\n",
    "pears_pred_wmat_avg = prediction_matrix(\n",
    "  FMat,\n",
    "  knn_tuple(similarity_matrix(FMat,pearson_similarity), k=3),\n",
    " weighted_average_prediction\n",
    ")\n",
    "pears_pred_wcmat_avg = prediction_matrix(\n",
    "    FMat,\n",
    "  knn_tuple(similarity_matrix(FMat,pearson_similarity), k=3),\n",
    " weighted_average_corrected_prediction\n",
    ")\n",
    "\n",
    "pdf = pd.DataFrame(\n",
    "  {\"avg-euc\": [pred_mat_avg[X_pers,X_Movie]], \n",
    "   \"wavg-euc\": [pred_wmat_avg[X_pers,X_Movie]], \n",
    "   \"wcavg-euc\": [pred_wcmat_avg[X_pers,X_Movie]],\n",
    "   \"avg-pearson\": [pears_pred_mat_avg[X_pers,X_Movie]], \n",
    "   \"wavg-pearson\": [pears_pred_wmat_avg[X_pers,X_Movie]], \n",
    "   \"wcavg-pearson\": [pears_pred_wcmat_avg[X_pers,X_Movie]]\n",
    "   }\n",
    ").transpose()\n",
    "print(prettier_dataframe_to_latex(pdf))\n",
    "\n",
    "\n",
    "# pred_mat_avg[X_pers,X_Movie]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
